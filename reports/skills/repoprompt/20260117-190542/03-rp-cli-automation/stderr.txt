OpenAI Codex v0.87.0-alpha.1 (research preview)
--------
workdir: /Users/jamiecraik/dev/agent-skills
model: gpt-5.2-codex
provider: openai
approval: never
sandbox: read-only
reasoning effort: medium
reasoning summaries: auto
session id: 019bcd5a-7a90-7933-a31f-8e3f44f6e057
--------
user
$repoprompt

How do I use rp-cli in automation?

mcp: openaiDeveloperDocs starting
mcp: pieces starting
mcp: local-memory starting
mcp: RepoPrompt starting
mcp: XcodeBuildMCP starting
mcp: RepoPrompt ready
mcp: local-memory ready
mcp: openaiDeveloperDocs ready
mcp: XcodeBuildMCP ready
mcp: pieces ready
mcp startup: ready: RepoPrompt, local-memory, openaiDeveloperDocs, XcodeBuildMCP, pieces

thinking
**Preparing to read skill documentation**
exec
/bin/zsh -lc ls in /Users/jamiecraik/dev/agent-skills succeeded in 52ms:
README.md
SKILL.md
apple
backend
build
frontend
github
product
scripts
skills
utilities

thinking
**Opening skill documentation**
exec
/bin/zsh -lc 'cat /Users/jamiecraik/dev/agent-skills/product/repoprompt/SKILL.md' in /Users/jamiecraik/dev/agent-skills succeeded in 51ms:
---
name: repoprompt
description: "Plan and guide integration of Repo Prompt (MCP, rp-cli, Compose/Chat/Apply) into AI coding workflows. Use when Codex needs to work with Repo Prompt for: (1) setup/integration, (2) workflow selection, (3) context optimization with codemaps/slices, or (4) comparisons with AI editors."
metadata:
  short-description: Integrate Repo Prompt into workflows
  version: "1.0.0"
  last_updated: "2026-01-17"
---

# Repo Prompt Integration

## Compliance
- Check against GOLD Industry Standards guide in ~/.codex/AGENTS.override.md

## Overview
Guide the user to the most effective Repo Prompt integration path for their workflow, with minimal setup friction and maximal context efficiency.

## When to Use
- User asks how to integrate Repo Prompt with Claude Code, Cursor, Codex, or other editors/agents.
- User asks how to use Compose vs Chat vs Apply/Pro Edit workflows.

thinking
**Locating reference file**

I'm identifying and reading the reference file needed for the next step.
exec
/bin/zsh -lc 'cat /Users/jamiecraik/dev/agent-skills/product/repoprompt/references/repoprompt_source.md' in /Users/jamiecraik/dev/agent-skills succeeded in 51ms:
# Repo Prompt Source Notes (Provided)

Use this as source material for summaries and explanations. Prefer concise paraphrase over long quotes. Avoid inventing pricing or feature availability; use what is here or ask.

## Overview

Repo Prompt helps build token-efficient, context-rich prompts so AI models understand code better. It targets the common problem of too little context (hallucinations) vs too much (token waste). Typical pain points:
- Manual copy/paste of files
- Blind agent discovery that burns tokens
- Paying per-token API costs for discovery

### Solution summary
1) Token-efficient context building
- Full content for active files
- Slices for targeted ranges
- Codemaps for signatures only

2) Use existing AI subscriptions via CLI providers
- Claude MAX / ChatGPT Plus or Pro / Google AI



thinking
**Outlining rp-cli automation usage**

I'm preparing a high-level guide on using rp-cli in automation, explaining its exec, interactive, and scripting modes as a proxy client via local sockets, plus a minimal workflow overview; next, I'll ask for the user's environment and constraints to tailor exact commands or steps.
thinking
**Preparing basic setup and smoke test**

I'm including a concise basic setup and smoke test aligned with skill anchors, confirming that /interview-me isn’t needed for this simple question, and omitting user profile reading as unnecessary here.
codex
Using repoprompt skill. You asked how to use `rp-cli` in automation; I’ll give a recommended automation flow and a minimal “basic setup and smoke test,” then ask for the one missing detail needed to give exact commands.

Objective: explain the rp-cli path for automation and how to validate it quickly.
Plan:
1) Confirm your automation context (CI vs local scripts) and target model provider.
2) Outline the rp-cli path and how it fits into an automation pipeline.
3) Provide a setup checklist and validation (smoke test) flow.
4) Show a minimal, tool-agnostic pipeline skeleton (inputs/outputs).
5) Ask for the missing detail to produce exact commands.

Next step: confirm where this runs (CI or local) and which model provider you use.

**rp-cli path (automation-focused)**
- rp-cli acts as a proxy MCP client via local sockets, and supports exec mode, interactive mode, and scripting.
- It’s best for non-MCP agents or shell pipelines where you want Repo Prompt’s Context Builder, codemaps/slices, and prompt exports as automation inputs.

**Recommended flow (automation)**
1) Use rp-cli to connect to Repo Prompt.
2) Run a discovery pass (Context Builder) or direct selection.
3) Build context using codemaps/slices where possible.
4) Export the prompt for your model call.
5) If you use XML edits, apply them via Repo Prompt’s Apply/Pro Edit workflow.

**setup checklist and validation**
- Confirm Repo Prompt is installed and your workspace opens cleanly.
- Ensure rp-cli is available in PATH.
- Run a basic setup and smoke test:
  - Select 2–3 files, build prompt, and export it.
  - Verify token count and that the prompt includes the selected files.
  - If using XML edits, do a tiny change round-trip (one file edit) and apply it.

**Context strategy**
- Full: files you will edit.
- Slices: large files where only sections matter.
- Codemaps: reference files and dependencies.

**Where I need one detail**
To give exact commands and a copy/paste pipeline, I need:
- Is this running in CI or local automation, and which model provider do you call (e.g., Claude/ChatGPT/Google AI)?

Action: Reply with “CI” or “local,” plus your model provider, and I’ll give exact rp-cli commands for your pipeline.
tokens used
5,694
