2026-01-17T19:00:01.168281Z ERROR codex_core::codex: failed to load skill /Users/jamiecraik/dev/agent-skills/product/interview-me/SKILL.md: missing YAML frontmatter delimited by ---
OpenAI Codex v0.87.0-alpha.1 (research preview)
--------
workdir: /Users/jamiecraik/dev/agent-skills
model: gpt-5.2-codex
provider: openai
approval: never
sandbox: read-only
reasoning effort: medium
reasoning summaries: auto
session id: 019bcd54-2012-78f0-92d7-02ddf9933e26
--------
user
$repoprompt

How do I use rp-cli in automation?

mcp: local-memory starting
mcp: openaiDeveloperDocs starting
mcp: RepoPrompt starting
mcp: XcodeBuildMCP starting
mcp: pieces starting
mcp: RepoPrompt ready
mcp: local-memory ready
mcp: openaiDeveloperDocs ready
mcp: XcodeBuildMCP ready
mcp: pieces ready
mcp startup: ready: RepoPrompt, local-memory, openaiDeveloperDocs, XcodeBuildMCP, pieces

thinking
**Preparing to read skill documentation**

I'm planning to read the SKILL.md file using an allowed command in the read-only sandbox to gather the necessary skill reprompt information.
exec
/bin/zsh -lc "ls /Users/jamiecraik/dev/agent-skills/skills/repoprompt && sed -n '1,200p' /Users/jamiecraik/dev/agent-skills/skills/repoprompt/SKILL.md" in /Users/jamiecraik/dev/agent-skills succeeded in 52ms:
SKILL.md
references
---
name: repoprompt
description: "Plan and guide integration of Repo Prompt (MCP, rp-cli, Compose/Chat/Apply) into AI coding workflows. Use when Codex needs to work with Repo Prompt for: (1) setup/integration, (2) workflow selection, (3) context optimization with codemaps/slices, or (4) comparisons with AI editors."
metadata:
  short-description: Integrate Repo Prompt into workflows
  version: "1.0.0"
  last_updated: "2026-01-17"
---

# Repo Prompt Integration

## Compliance
- Check against GOLD Industry Standards guide in ~/.codex/AGENTS.override.md

## Overview
Guide the user to the most effective Repo Prompt integration path for their workflow, with minimal setup friction and maximal context efficiency.

## When to Use

thinking
**Opening reference file**
exec
/bin/zsh -lc "sed -n '1,220p' /Users/jamiecraik/dev/agent-skills/skills/repoprompt/references/repoprompt_source.md" in /Users/jamiecraik/dev/agent-skills succeeded in 51ms:
# Repo Prompt Source Notes (Provided)

Use this as source material for summaries and explanations. Prefer concise paraphrase over long quotes. Avoid inventing pricing or feature availability; use what is here or ask.

## Overview

Repo Prompt helps build token-efficient, context-rich prompts so AI models understand code better. It targets the common problem of too little context (hallucinations) vs too much (token waste). Typical pain points:
- Manual copy/paste of files
- Blind agent discovery that burns tokens
- Paying per-token API costs for discovery

### Solution summary
1) Token-efficient context building
- Full content for active files
- Slices for targeted ranges
- Codemaps for signatures only

2) Use existing AI subscriptions via CLI providers
- Claude MAX / ChatGPT Plus or Pro / Google AI


thinking
**Summarizing response plan**

The objective is to guide the user on using rp-cli automation accurately without assuming paths or inventing flags. The plan is to explain the general command pattern, ask for the installed version or `rp --help` output to ensure accuracy, provide explicit step-by-step instructions, mention constraints, and offer recommendations plus alternatives. The next step is to share a concise, clear checklist and request the `rp --help` output for precise guidance.
codex
Objective: explain how to use `rp-cli` for automation in your pipeline, using Repo Prompt as a context builder/exporter.

Plan:
1) Confirm your automation environment (CI vs local script) and how you want to pass prompts to models.
2) Clarify `rp-cli` install/version and available subcommands.
3) Outline a minimal automation flow (select context → build prompt → export → send to model).
4) Provide a smoke-test script stub tailored to your environment.
5) Add token-efficiency guidance (full vs slices vs codemaps).
6) Offer next-step options and a recommendation.

Next step to execute: verify `rp-cli` subcommands so I can give exact commands.

How `rp-cli` fits automation (from Repo Prompt notes)
- `rp-cli` acts as a proxy MCP client via local sockets.
- It supports exec mode, interactive mode, and scripting.
- It’s useful for agents or pipelines that can’t use MCP directly.

Recommended automation flow (generic, safe)
1) Open workspace context (select files / slices / codemaps).
2) Build prompt (Compose-style).
3) Export prompt to stdout or file.
4) Send to model (API/CLI provider).
5) (Optional) Apply XML edits with Apply/Pro Edit if the model returns them.

Context strategy (token-efficient defaults)
- Full: only files you will edit.
- Slices: large files where only ranges matter.
- Codemaps: dependencies and reference files.

My read: you want a scriptable “build prompt → run model → (optional) apply edits” loop. Correct?

Action: please paste the output of `rp --help` (or `rp-cli --help`) so I can give exact command lines.
tokens used
27,512
